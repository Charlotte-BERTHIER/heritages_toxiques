{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap des activités des anciens sites industriels CASIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/58.0.3029.110 Safari/537.3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_activites(url_bdd_casias, headers):\n",
    "\n",
    "    \"\"\"\n",
    "    Fonction pour scrapper les codes activités NAF 2008 des sites répertoriés dans CASIAS. \n",
    "    La fonction prend en entrée l'url d'un seul site. \n",
    "    Elle retourne la liste des codes NAF 2008 alphanumérique pour ce site. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    url_erreurs = []\n",
    "    \n",
    "    try : \n",
    "        # Récupérer l'information \n",
    "        response = requests.get(url_bdd_casias, headers=headers)   \n",
    "\n",
    "        # Ensuite j'utilise BeautifulSoup pour récupérer les éléments HTML (parser)\n",
    "        soupe = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Trouver la section \"Activités de l'établissement\"\n",
    "        activites_section = soupe.find('h3', string=\"3.- Activités de l'établissement\")\n",
    "        # Si elle existe\n",
    "        if activites_section:\n",
    "            # Trouver le tableau dans la section \"Activités de l'établissement\"\n",
    "            activites_table = activites_section.find_next('table', class_='data')\n",
    "\n",
    "            if activites_table:\n",
    "                # Initialiser une liste pour stocker les données des activités secondaires\n",
    "                activites_secondaires = []\n",
    "\n",
    "                # Parcourir les lignes du tableau\n",
    "                for row in activites_table.find_all('tr'):\n",
    "                    for cell in row.find_all('td'):\n",
    "                        # Ajouter le texte de la cellule à la liste des activités secondaires\n",
    "                        activites_secondaires.append(cell.get_text(strip=True))    \n",
    "\n",
    "            else:\n",
    "                print(\"Aucun tableau trouvé dans la section 'Activités de l'établissement'\")\n",
    "        else:\n",
    "            print(\"Aucune section 'Activités de l'établissement' trouvée dans le document HTML\")\n",
    "        \n",
    "        # Définition de la RegEx pour récupérer uniquement le code d'activité\n",
    "        regex_tiret = \"(.+?)\\s-\\s\"\n",
    "\n",
    "        # Amélioration\n",
    "        liste_propre = []\n",
    "\n",
    "        for element in activites_secondaires:\n",
    "            liste_propre.extend(re.findall(regex_tiret,element))\n",
    "\n",
    "        return liste_propre\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Gérer les erreurs liées aux requêtes HTTP\n",
    "        print(\"Une erreur s'est produite lors de la récupération de la page :\", e)\n",
    "        url_erreurs.append(url_bdd_casias)  # Ajouter l'URL ayant rencontré une erreur à la liste\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        # Gérer toutes les autres erreurs\n",
    "        print(\"Une erreur s'est produite :\", e)\n",
    "        url_erreurs.append(url_bdd_casias)  # Ajouter l'URL ayant rencontré une erreur à la liste\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyer_nom_colonne(texte):\n",
    "    # Supprimer les accents\n",
    "    texte_sans_accents = ''.join(char for char in unicodedata.normalize('NFD', texte) if unicodedata.category(char) != 'Mn')\n",
    "    # Remplacer la ponctuation par des espaces\n",
    "    texte_sans_ponctuation = re.sub(r'[^\\w\\s]', ' ', texte_sans_accents)\n",
    "    # Mettre en minuscules\n",
    "    texte_minuscules = texte_sans_ponctuation.lower()\n",
    "    # Remplacer les espaces par des underscores\n",
    "    texte_avec_underscores = re.sub(r'\\s+', '_', texte_minuscules)\n",
    "    return texte_avec_underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretraitements_df_pollution_BRGM(input_bdd_polluants):\n",
    "   \n",
    "    df_pollution = pd.read_csv(input_bdd_polluants, sep=';')\n",
    "    \n",
    "    # Nettoyage pollution \n",
    "    df_pollution = df_pollution.fillna(0)\n",
    "    df_pollution['Gaz du sol'] = df_pollution['Gaz du sol'].map(lambda x: x == \"X\")\n",
    "    df_pollution['Sol'] = df_pollution['Sol'].map(lambda x: x == \"X\")\n",
    "    df_pollution['eaux souterraines'] = df_pollution['eaux souterraines'].map(lambda x: x == \"X\")\n",
    "    \n",
    "    # Nettoyage noms colonnes\n",
    "    df_pollution.rename(columns = lambda x: nettoyer_nom_colonne(x), inplace= True)\n",
    "\n",
    "    return df_pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretraitement_df_CASIAS(input_bdd_casias):\n",
    "    \"\"\"\n",
    "    Prétraitement de la BDD CASIAS\n",
    "\n",
    "    Suppression des sites non géolocalisés dès le départ afin d'alléger le scrapping. On conserve les doublons de points. \n",
    "\n",
    "    Entrée : lien vers la BDD en local\n",
    "    Sortie : dataframe propre données CASIAS\n",
    "\n",
    "    \"\"\"\n",
    "    df_casias_IDF = pd.read_csv(input_bdd_casias, sep=';')\n",
    "    df_casias_IDF = df_casias_IDF.fillna(0)\n",
    "    df_casias_IDF['code_postal'] = df_casias_IDF['code_postal'].astype(int)\n",
    "    regex_ponctuation = r'[.,;:?!«»(){}\\[\\]\\'\"\\-…]'\n",
    "    df_casias_IDF['nom_etablissement'] = df_casias_IDF['nom_etablissement'].str.replace(regex_ponctuation, '')\n",
    "    df_casias_IDF = df_casias_IDF.fillna(0)\n",
    "    df_casias_IDF = df_casias_IDF.loc[df_casias_IDF['nature_localisation'] != \"Site non gélocalisé\"]\n",
    "\n",
    "    return df_casias_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothèses de pollution\n",
    "Croisement des BDD CASIAS et ActiviPol v3 pour obtenir les hypothèses de pollution associé à chaque site industriel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypotheses_pollution_usine(liste_activites_usine, df_pollution):\n",
    "\n",
    "    \"\"\"\n",
    "    Fonction pour récupérer les hypothèses de pollution associées à un code NAF 2008. \n",
    "    On cherche une note unique de pollution pour chaque groupe de substances.\n",
    "    Si un site a plusieurs activités, on garde l'hypothèse de pollution la plus élevée. \n",
    "\n",
    "        Entrées : \n",
    "            liste des codes NAF 2008 d'un site CASIAS \n",
    "            dataframe BRGM avec les hypothèses de pollution des groupes de substances\n",
    "\n",
    "        Sortie : \n",
    "            pour 1 site CASIAS\n",
    "            dictionnaire avec les hypothèses de pollution par activités, globales et les indicateurs agrégés\n",
    "    \"\"\"\n",
    "      \n",
    "    # Extraction des polluants associés à chaque activité dans le df de corrélation BRGM\n",
    "    dict_NAF2008_Pol = {}\n",
    "        \n",
    "    for code in liste_activites_usine:\n",
    "            try : \n",
    "                # Extraction des notes associées à l'activité\n",
    "                contenu = df_pollution[code].tolist()\n",
    "                \n",
    "                # Stockage dans un dictionnaire\n",
    "                dict_NAF2008_Pol[code] = {\"values\": contenu}\n",
    "\n",
    "            except KeyError:\n",
    "            # Gérer l'erreur lorsque la colonne correspondant au code NAF 2008 n'est pas trouvée dans le DataFrame df_pollution\n",
    "                print(f\"La colonne correspondant au code {code} n'existe pas dans le DataFrame df_pollution.\")\n",
    "        \n",
    "            except Exception as e:\n",
    "            # Gérer toutes les autres exceptions\n",
    "                 print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "    # On ne conserve que la note la plus haute pour chaque substance (regroupement au niveau de l'usine)\n",
    "    resultats_filtres = {}\n",
    "\n",
    "    # Parcourir chaque identifiant dans le dictionnaire de résultats\n",
    "    for identifiant, valeurs in dict_NAF2008_Pol.items():\n",
    "        # Si c'est le premier identifiant, copier simplement les valeurs\n",
    "        if not resultats_filtres:\n",
    "            resultats_filtres = valeurs\n",
    "        else:\n",
    "            # Parcourir chaque clé de colonne dans les valeurs du résultat\n",
    "            for colonne, valeur in valeurs.items():\n",
    "                # Sélectionner la valeur la plus grande entre la valeur actuelle et la valeur précédente\n",
    "                resultats_filtres[colonne] = max(resultats_filtres[colonne], valeur)\n",
    "                \n",
    "    # Ajout des valeurs simplifiées dans le dictionnaire de résultats \n",
    "    dict_NAF2008_Pol['valeurs_new'] = resultats_filtres['values']\n",
    "\n",
    "    # Compter le nombre total de substances potentiellement présentes sur le site \n",
    "    nb_substances = len([nombre for nombre in dict_NAF2008_Pol.get('valeurs_new') if nombre != 0])\n",
    "    dict_NAF2008_Pol['nb_substances'] = nb_substances\n",
    "    \n",
    "    # Note moyenne pollution totale\n",
    "    valeurs_non_nulles = [nombre for nombre in dict_NAF2008_Pol.get('valeurs_new') if nombre != 0]\n",
    "    dict_NAF2008_Pol['m_totale'] = sum(valeurs_non_nulles) / nb_substances\n",
    "\n",
    "    # Note moyenne produits chimiques \n",
    "    try:\n",
    "        # Tentative de calcul de la moyenne\n",
    "        dict_NAF2008_Pol[\"m_chimique\"] = sum([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][0:3] if nombre != 0])/len([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][0:3] if nombre != 0])\n",
    "    except ZeroDivisionError:\n",
    "        dict_NAF2008_Pol[\"m_chimique\"] = 0\n",
    "\n",
    "    try : \n",
    "        dict_NAF2008_Pol['m_elem_mineraux'] = sum([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][3:8] if nombre != 0])/len([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][3:8] if nombre != 0])\n",
    "    except ZeroDivisionError:\n",
    "        dict_NAF2008_Pol[\"m_elem_mineraux\"] = 0\n",
    "        \n",
    "    try :\n",
    "        dict_NAF2008_Pol['m_ETM'] = sum([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][8:23] if nombre != 0])/len([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][8:23] if nombre != 0])\n",
    "    except ZeroDivisionError:\n",
    "        dict_NAF2008_Pol[\"m_ETM\"] = 0\n",
    "\n",
    "    try :\n",
    "        dict_NAF2008_Pol['m_organiques'] = sum([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][23:55] if nombre != 0])/len([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][23:55] if nombre != 0])\n",
    "    except ZeroDivisionError:\n",
    "        dict_NAF2008_Pol[\"m_organiques\"] = 0  \n",
    "    \n",
    "    try:\n",
    "        dict_NAF2008_Pol['m_pharmaceutiques'] = sum([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][55:64] if nombre != 0])/len([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][55:64] if nombre != 0])\n",
    "    except ZeroDivisionError:\n",
    "        dict_NAF2008_Pol[\"m_pharmaceutiques\"] = 0\n",
    "    \n",
    "    try: \n",
    "        dict_NAF2008_Pol['m_phytosanitaires'] = sum([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][64:73] if nombre != 0])/len([nombre for nombre in dict_NAF2008_Pol['valeurs_new'][64:73] if nombre != 0])\n",
    "    except ZeroDivisionError:\n",
    "        dict_NAF2008_Pol[\"m_phytosanitaires\"] = 0\n",
    "        \n",
    "    return dict_NAF2008_Pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boucle_hyp_pollution_all_casias(df_casias, df_pollution, headers): \n",
    "    \"\"\"\n",
    "    Fonction pour scrapper les activités, et récupérer les hypothèses de pollution correspondates sur toutes la BDD Casias.\n",
    "\n",
    "    Entrée : \n",
    "        BDD Casias\n",
    "        BDD BRGM pollution\n",
    "        headers scrapping\n",
    "        \n",
    "    Sortie :  dictionnaire avec les données pour toute la BDD Casias \n",
    "    \"\"\"\n",
    "\n",
    "    dict_final_hyp_pollution = {}\n",
    "\n",
    "    # Pour chaque entrée de la BDD CASIAS (anciens sites industriels)\n",
    "    for index, row in df_casias.iterrows():\n",
    "        code_usine = row['code_metier']\n",
    "        \n",
    "        # Récupération des codes activités NAF 2008 via la fiche de site en ligne \n",
    "        # sortie : liste des codes alphanumériques NAF 2008 associés\n",
    "        usine_url = row['fiche_risque']\n",
    "        activites_usine = scrap_activites(usine_url, headers)\n",
    "\n",
    "        if activites_usine:\n",
    "            # Extraction des hypothèses de pollution du site \n",
    "            # sortie : dictionnaire activité / pollution \n",
    "            try:\n",
    "                dict_hyp_pollution_usine = hypotheses_pollution_usine(activites_usine, df_pollution)\n",
    "                dict_final_hyp_pollution[code_usine] = dict_hyp_pollution_usine\n",
    "            \n",
    "            except KeyError:\n",
    "                # Gérer l'erreur lorsque la colonne correspondant au code NAF 2008 n'est pas trouvée dans le DataFrame df_pollution\n",
    "                print(f\"L'un des codes activités n'a pas été trouvé le DataFrame df_pollution.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Gérer toutes les autres exceptions\n",
    "                print(f\"Une erreur s'est produite : {e}\")\n",
    "                \n",
    "        \n",
    "        else: \n",
    "            pass\n",
    "\n",
    "        \n",
    "    return dict_final_hyp_pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_results_vers_df_pollution(dict_final_hyp_pollution, df_casias, df_pollution):\n",
    "\n",
    "    \"\"\"\n",
    "    Fonction pour ajouter les hypothèses de pollution au dataframe Casias initial. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Valeurs max par substances\n",
    "    df_SSP_pollution = pd.DataFrame([value['valeurs_new'] for value in dict_final_hyp_pollution.values()])\n",
    "\n",
    "    # Colonnes\n",
    "    sous_groupes = list(df_pollution['Sous-groupe de substances'])\n",
    "    df_SSP_pollution.columns = sous_groupes\n",
    "\n",
    "    # ID \n",
    "    df_SSP_pollution['code_metier'] = dict_final_hyp_pollution.keys()\n",
    "\n",
    "    # Indices de pollution aggrégés \n",
    "    df_SSP_pollution['m_totale'] = [value['m_totale'] for value in dict_final_hyp_pollution.values()]\n",
    "    df_SSP_pollution['nb_substances'] = [value['nb_substances'] for value in dict_final_hyp_pollution.values()]\n",
    "    df_SSP_pollution['m_chimique'] = [value['m_chimique'] for value in dict_final_hyp_pollution.values()]\n",
    "    df_SSP_pollution['m_elem_mineraux'] = [value['m_elem_mineraux'] for value in dict_final_hyp_pollution.values()]\n",
    "    df_SSP_pollution['m_ETM'] = [value['m_ETM'] for value in dict_final_hyp_pollution.values()]\n",
    "    df_SSP_pollution['m_organiques'] = [value['m_organiques'] for value in dict_final_hyp_pollution.values()]\n",
    "    df_SSP_pollution['m_pharmaceutiques'] = [value['m_pharmaceutiques'] for value in dict_final_hyp_pollution.values()]\n",
    "    df_SSP_pollution['m_phytosanitaires'] = [value['m_phytosanitaires'] for value in dict_final_hyp_pollution.values()]\n",
    "    \n",
    "    # Nouveau df CASIAS avec les valeurs de pollution par usine \n",
    "    df_casias_pollution = pd.merge(df_casias, df_SSP_pollution, how='outer', on='code_metier')\n",
    "   \n",
    "    return df_casias_pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation finale de toutes les fonctions\n",
    "\n",
    "# Charger les datasets \n",
    "url_bdd_casias_IDF = \"/Users/charlotteberthier/Documents/Memoire/9_Git_heritages_toxiques/source/CASIAS/240422_CASIAS_IDF.csv\"\n",
    "url_bdd_casias_93 = \"/Users/charlotteberthier/Documents/Memoire/9_Git_heritages_toxiques/source/CASIAS/240424_CASIAS_93.csv\"\n",
    "url_bdd_polluants = \"/Users/charlotteberthier/Documents/Mémoire/Production/EDV_Exploration_des_données/Data intermédiaire/Corr_sousgrpe_polluants_activités.csv\"\n",
    "\n",
    "df_pollution = pretraitements_df_pollution_BRGM(url_bdd_polluants)\n",
    "df_casias = pretraitement_df_CASIAS(url_bdd_casias_IDF)\n",
    "\n",
    "# Boucle sur tout CASIAS \n",
    "dict_final_hyp_pollution = boucle_hyp_pollution_all_casias(df_casias, df_pollution, headers)\n",
    "\n",
    "# Conversion du dictionnaire de résultats vers CASIAS \n",
    "\n",
    "df_final_casias_pollution = merge_results_vers_df_pollution(dict_final_hyp_pollution, df_casias, df_pollution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_casias_pollution.to_csv('df_final_casias_pollution.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advancedpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
